Compare the two C4 documentation sets in this project and evaluate which is better quality, more accurate, and more useful.

## Two Documentation Sets

1. **Root set:** `C4-Documentation/` (in project root)
2. **Docs set:** `docs/C4-Documentation/` (under docs/)

These were generated by different versions of the C4 documentation prompt. Your job is to do a thorough comparative analysis.

## What to Compare

### 1. Inventory
List every file in each set. Note which files exist in one but not the other.

### 2. Accuracy (most important)
For EACH C4 level in BOTH sets, cross-reference against actual source code in `src/` and `tests/`:
- Are function signatures correct? Check actual files.
- Are dependencies accurately mapped?
- Are class/module hierarchies right?
- Are any functions/classes missing or phantom (documented but don't exist)?

### 3. C4 Model Compliance
For each set, evaluate:
- **Context level:** Does it focus on people and systems without technology details leaking in?
- **Container level:** Does it show actual deployment units with correct technology choices?
- **Component level:** Are boundaries logical? Is every code file assigned?
- **Code level:** Are signatures complete? Are diagram types appropriate for the paradigm?

### 4. Completeness
- Which set covers more of the codebase?
- Which has better cross-referencing between levels?
- Which has more useful Mermaid diagrams?

### 5. Usability
- Which README is more navigable?
- Which set would be more useful for developer onboarding?
- Which has better writing quality (concise, clear, no fluff)?

### 6. Structural Differences
- Different component groupings?
- Different container identification?
- Different persona modeling?
- Different user journeys?

## Output Requirements

Create findings in comms/outbox/exploration/c4-output-comparison/:

### README.md (required)
First paragraph: Clear verdict — which set is better overall and why.

Then:
- **Winner by Category:** table showing which set wins each dimension
- **Key Differences:** the most significant divergences between the two sets
- **Accuracy Scorecard:** summary of cross-reference checks against source code

### accuracy-comparison.md
Detailed accuracy analysis. For each C4 level in both sets, list specific claims checked against source code. Include file paths and line numbers. Flag any errors found in either set.

### structure-comparison.md
Side-by-side comparison of how each set models the architecture — different components, containers, personas, boundaries. Evaluate which modeling choices are better justified.

### recommendations.md
What should be kept from the better set, what's worth salvaging from the worse set, and what improvements neither set achieved.

## Guidelines
- Be direct. Pick winners. Don't hedge with "both are good."
- Include specific evidence for every claim (file paths, function names, line numbers).
- If both sets make the same mistake, call it out.
- If one set has something genuinely better than the other, explain why concretely.

## When Complete
git add comms/outbox/exploration/c4-output-comparison/
git commit -m "exploration: c4-output-comparison complete"
